{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction de Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraire les mots clés d'un document avec Yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yake.yake.KeywordExtractor at 0x7fc807115970>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lister les Fichiers\n",
    "data_path = \"../data/tmp/\"\n",
    "files = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimer le nombre de fichiers identifiés\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1928.txt',\n",
       " '1961.png',\n",
       " '1945_clean.txt',\n",
       " '1928.png',\n",
       " '1961.txt',\n",
       " '1928_clean.txt',\n",
       " '1945.txt',\n",
       " '1961_clean.txt',\n",
       " '1945.png']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Les dix premiers fichiers\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1961.txt']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choisir un fichier\n",
    "this_file = files[4:5]\n",
    "this_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A SAMEDI .30 DECEMBRE 1961 LE som 15 /, n^” d«m ' . av. *nf, sorvntite fi7.78.39. 1P8025K rî, clinique Ho»- Jflslll P, 83.78 39 SfTsrvsnte «i Int. mén. UrKcnt. Téléphone img. inllêre A Ocnvnl. 108911K externe 2 en tin MEDECIN Dm*, dem. concierge pr cnli. médtc. veuve, pensionnée ou mén M enf mnrl tr.dch. T. 43.09.82. 108870k Mén. fionil TnC ësl dent, pr 418 nv. de l'Exposition, Bruxel. 9 • Téléph. le mntln VÔÜB3BK 78.00.30. 1870S8K JEUNE FILLE nmiv.-né tel. Bhode ... rfnm. pour s'occuper *■ » in\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Récupérer le texte du fichier\n",
    "text = open(os.path.join(data_path, '1961.txt'), 'r', encoding='utf-8').read()\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ecrire Agence Rossel', 2.7399389594775735e-06),\n",
       " ('Tél', 1.6704782544917675e-05),\n",
       " ('Bruxelles', 1.6944416722879466e-05),\n",
       " ('Agence Rossel', 2.0563123606799485e-05),\n",
       " ('rue', 2.6638241002519392e-05),\n",
       " ('BRUXELLES tél', 2.78309606420775e-05),\n",
       " ('WATERLOO Bruxelles tél', 2.9889396342982175e-05),\n",
       " ('BRUXELLES demande Bonne', 3.6752504392065566e-05),\n",
       " ('Rossel', 4.057199107239906e-05),\n",
       " ('New York', 5.378970202728555e-05),\n",
       " ('Agence Rossel num', 5.977453888773685e-05),\n",
       " ('rue Van', 6.172166981750555e-05),\n",
       " ('Ecrire Agence', 6.939176058952134e-05),\n",
       " ('Prix', 7.259539103402078e-05),\n",
       " ('Van', 7.596477965363498e-05),\n",
       " ('Brux', 8.839328681993052e-05),\n",
       " ('charbon BRUXELLES tél', 8.990651428934504e-05),\n",
       " ('Bruxelles vers Anvers', 9.618153347240282e-05),\n",
       " ('BRUXELLES demande', 9.957872845587265e-05),\n",
       " ('rue Jean Van', 0.00010501378587738326),\n",
       " ('dem', 0.00011085469349188376),\n",
       " ('Voir Bruxelles français', 0.00011174929284727355),\n",
       " ('VAN DEN PLAS', 0.00011569214356430782),\n",
       " ('l’Agence Rossel', 0.00011738147718739506),\n",
       " ('ans', 0.00011985580969570679),\n",
       " ('Agence Rossel numéro', 0.00012075526640972291),\n",
       " ('Ecrire', 0.00012704866190788738),\n",
       " ('Importante Société Bruxelles', 0.00014407438112542322),\n",
       " ('Centre Bruxelles petit', 0.0001535477163553198),\n",
       " ('app', 0.0001540941688987866),\n",
       " ('Van den', 0.00015650420981741203),\n",
       " ('rue Lesbroussart Tél', 0.0001651381459899738),\n",
       " ('cours', 0.00016638128590883201),\n",
       " ('BRUXELLES demande EMPLOYE', 0.00017021934686547926),\n",
       " ('soir', 0.00017180893188098456),\n",
       " ('Anvers', 0.00017736677172269056),\n",
       " ('rue Royale', 0.00017940114405848407),\n",
       " ('NEW YORK COURS', 0.00018799371770394693),\n",
       " ('Belgique', 0.00020457866247353858),\n",
       " ('RUE TENBOSCH Tél', 0.00021828269562950765),\n",
       " ('FIRME IMPORTANTE BRUXELLES', 0.000220755440536185),\n",
       " ('Faire offre', 0.00022262123635134894),\n",
       " ('offre bon prix', 0.00022572071355483704),\n",
       " ('bon', 0.000227909314946917),\n",
       " ('rue Ducale Tél', 0.00022815150727324167),\n",
       " ('BRUXELLES cherche JEUNE', 0.00022926221213008952),\n",
       " ('BON ETAT Tél', 0.00023299874210878105),\n",
       " ('Bel app', 0.00023677482510439592),\n",
       " ('Bruxelles dem', 0.0002413080936900046),\n",
       " ('Bruxelles EMPLOYEE DACTYLO', 0.00024250624695535948)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraire les mots clés de ce texte\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Agence Rossel',\n",
       " 'BRUXELLES tél',\n",
       " 'New York',\n",
       " 'rue Van',\n",
       " 'Ecrire Agence',\n",
       " 'BRUXELLES demande',\n",
       " 'l’Agence Rossel',\n",
       " 'Van den',\n",
       " 'rue Royale',\n",
       " 'Faire offre',\n",
       " 'Bel app',\n",
       " 'Bruxelles dem']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ne garder que les bigrammes\n",
    "kept = []\n",
    "for kw, score in keywords:\n",
    "    words = kw.split()\n",
    "    if len(words) == 2:\n",
    "        kept.append(kw)\n",
    "kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire la même opération sur tous les documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [67], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(files)[:\u001b[39m10\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, \u001b[39m'\u001b[39m\u001b[39m1928_clean.txt\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mread()\n\u001b[0;32m----> 3\u001b[0m     keywords \u001b[39m=\u001b[39m kw_extractor\u001b[39m.\u001b[39;49mextract_keywords(text)\n\u001b[1;32m      4\u001b[0m     kept \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m kw, score \u001b[39min\u001b[39;00m keywords:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/yake/yake.py:64\u001b[0m, in \u001b[0;36mKeywordExtractor.extract_keywords\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[1;32m     63\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m dc \u001b[39m=\u001b[39m DataCore(text\u001b[39m=\u001b[39;49mtext, stopword_set\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstopword_set, windowsSize\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindowsSize, n\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn)\n\u001b[1;32m     65\u001b[0m dc\u001b[39m.\u001b[39mbuild_single_terms_features(features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[1;32m     66\u001b[0m dc\u001b[39m.\u001b[39mbuild_mult_terms_features(features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/yake/datarepresentation.py:30\u001b[0m, in \u001b[0;36mDataCore.__init__\u001b[0;34m(self, text, stopword_set, windowsSize, n, tagsToDiscard, exclude)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfreq_ns[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstopword_set \u001b[39m=\u001b[39m stopword_set\n\u001b[0;32m---> 30\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build(text, windowsSize, n)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/yake/datarepresentation.py:50\u001b[0m, in \u001b[0;36mDataCore._build\u001b[0;34m(self, text, windowsSize, n)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build\u001b[39m(\u001b[39mself\u001b[39m, text, windowsSize, n):\n\u001b[1;32m     49\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_filter(text)\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_str \u001b[39m=\u001b[39m [ [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m split_contractions(web_tokenizer(s)) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (w\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(w) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(w) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(split_multi(text)) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(s\u001b[39m.\u001b[39mstrip()) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumber_of_sentences \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_str)\n\u001b[1;32m     52\u001b[0m     pos_text \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/yake/datarepresentation.py:50\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build\u001b[39m(\u001b[39mself\u001b[39m, text, windowsSize, n):\n\u001b[1;32m     49\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_filter(text)\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_str \u001b[39m=\u001b[39m [ [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m split_contractions(web_tokenizer(s)) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (w\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(w) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(w) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(split_multi(text)) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(s\u001b[39m.\u001b[39mstrip()) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumber_of_sentences \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences_str)\n\u001b[1;32m     52\u001b[0m     pos_text \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/segtok/tokenizer.py:306\u001b[0m, in \u001b[0;36mweb_tokenizer\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m@_matches\u001b[39m(\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[39m    (?<=^|[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms<\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39m\u001b[39m])            # visual border\u001b[39m\n\u001b[1;32m    282\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweb_tokenizer\u001b[39m(sentence):\n\u001b[1;32m    302\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m    The web tokenizer works like the :func:`word_tokenizer`, but does not split URIs or\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m    e-mail addresses. It also un-escapes all escape sequences (except in URIs or email addresses).\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m i, span \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(web_tokenizer\u001b[39m.\u001b[39msplit(sentence))\n\u001b[1;32m    307\u001b[0m             \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m ((span,) \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m word_tokenizer(unescape(span)))]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/segtok/tokenizer.py:307\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39m@_matches\u001b[39m(\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    281\u001b[0m \u001b[39m    (?<=^|[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms<\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39m\u001b[39m])            # visual border\u001b[39m\n\u001b[1;32m    282\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweb_tokenizer\u001b[39m(sentence):\n\u001b[1;32m    302\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m    The web tokenizer works like the :func:`word_tokenizer`, but does not split URIs or\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m    e-mail addresses. It also un-escapes all escape sequences (except in URIs or email addresses).\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m i, span \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(web_tokenizer\u001b[39m.\u001b[39msplit(sentence))\n\u001b[0;32m--> 307\u001b[0m             \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m ((span,) \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m word_tokenizer(unescape(span)))]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/segtok/tokenizer.py:237\u001b[0m, in \u001b[0;36mword_tokenizer\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39mThis tokenizer extends the alphanumeric :func:`symbol_tokenizer` by splitting fewer cases:\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m6. Subscript digits are attached if prefixed with letters that look like a chemical formula.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m pruned \u001b[39m=\u001b[39m HYPHENATED_LINEBREAK\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m, sentence)\n\u001b[0;32m--> 237\u001b[0m tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m span \u001b[39min\u001b[39;00m space_tokenizer(pruned) \u001b[39mfor\u001b[39;00m\n\u001b[1;32m    238\u001b[0m           token \u001b[39min\u001b[39;00m word_tokenizer\u001b[39m.\u001b[39msplit(span) \u001b[39mif\u001b[39;00m token]\n\u001b[1;32m    240\u001b[0m \u001b[39m# splice the sentence terminal off the last word/token if it has any at its borders\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39m# only look for the sentence terminal in the last three tokens\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39mfor\u001b[39;00m idx, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mreversed\u001b[39m(tokens[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:]), \u001b[39m1\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/segtok/tokenizer.py:238\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39mThis tokenizer extends the alphanumeric :func:`symbol_tokenizer` by splitting fewer cases:\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m6. Subscript digits are attached if prefixed with letters that look like a chemical formula.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m pruned \u001b[39m=\u001b[39m HYPHENATED_LINEBREAK\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m, sentence)\n\u001b[1;32m    237\u001b[0m tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m span \u001b[39min\u001b[39;00m space_tokenizer(pruned) \u001b[39mfor\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m           token \u001b[39min\u001b[39;00m word_tokenizer\u001b[39m.\u001b[39;49msplit(span) \u001b[39mif\u001b[39;00m token]\n\u001b[1;32m    240\u001b[0m \u001b[39m# splice the sentence terminal off the last word/token if it has any at its borders\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39m# only look for the sentence terminal in the last three tokens\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39mfor\u001b[39;00m idx, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mreversed\u001b[39m(tokens[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:]), \u001b[39m1\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for f in sorted(files)[:10]:\n",
    "    text = open(os.path.join(data_path, '1928_clean.txt'), 'r', encoding=\"utf-8\").read()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    kept = []\n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 2:\n",
    "            kept.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept)}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
